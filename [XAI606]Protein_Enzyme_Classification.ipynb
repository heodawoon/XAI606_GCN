{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[XAI606]Protein_Enzyme_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMdPsw3oPDoY3eQO6jGBhJQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heodawoon/XAI606_GCN/blob/main/%5BXAI606%5DProtein_Enzyme_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-a7p3u6HngGp"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-R1crEPRlwpz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48f3067c-1104-4721-eb2d-9ed2cf01c0a8"
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.version.cuda)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.10.0+cu111\n",
            "11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEtVQ_SsnaYt",
        "outputId": "49ff85f2-4399-4d9c-b084-f1e1c51fcca7"
      },
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
        "!pip install torch-geometric"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.7/dist-packages (2.0.9)\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.7/dist-packages (0.6.12)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.19.5)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.7/dist-packages (2.0.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.13)\n",
            "Requirement already satisfied: yacs in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.1.8)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (6.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.1.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.6)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.6.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.62.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (0.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (57.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex-oicrtn5ce"
      },
      "source": [
        "import random\n",
        "\n",
        "import os.path as osp\n",
        "from math import ceil\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear\n",
        "import torch_geometric\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GCNConv, DenseGraphConv, dense_diff_pool, TopKPooling, dense_mincut_pool #, GATConv, GraphUNet\n",
        "from torch_geometric.utils import to_dense_batch, to_dense_adj"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ushPHEImn7VT"
      },
      "source": [
        "# torch.manual_seed(920)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4ABaJjnkI7h"
      },
      "source": [
        "path = \"/content\"\n",
        "dataset = TUDataset(path, name='PROTEINS').shuffle()\n",
        "n = len(dataset)\n",
        "average_nodes = int(dataset.data.x.size(0) / len(dataset))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NV8IhGJFc9kD",
        "outputId": "4d6430e6-dab6-48dc-aa0b-a8cd69e5626d"
      },
      "source": [
        "# random.seed(10) # Iteration 1\n",
        "# random.seed(20) # Iteration \n",
        "# random.seed(30) # Iteration 3\n",
        "# random.seed(40) # Iteration 4\n",
        "random.seed(50) # Iteration 5\n",
        "random_index = random.sample(range(len(dataset)), len(dataset))\n",
        "print(random_index)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1018, 545, 745, 496, 969, 675, 174, 1102, 649, 459, 1106, 313, 711, 201, 1100, 654, 455, 387, 140, 673, 878, 182, 131, 684, 881, 11, 465, 691, 1111, 889, 216, 411, 906, 555, 942, 259, 552, 428, 251, 843, 240, 1016, 874, 1059, 256, 1022, 1057, 615, 190, 753, 1058, 307, 13, 733, 18, 841, 968, 1103, 332, 200, 280, 524, 770, 503, 688, 68, 96, 161, 485, 536, 862, 952, 856, 800, 962, 1091, 825, 248, 742, 105, 39, 138, 908, 1062, 629, 648, 710, 1019, 181, 617, 163, 440, 1099, 948, 1012, 114, 674, 90, 1087, 1067, 135, 51, 1005, 146, 30, 318, 702, 755, 864, 981, 364, 16, 183, 512, 54, 246, 593, 920, 724, 315, 714, 603, 27, 152, 505, 413, 442, 356, 1076, 156, 228, 875, 990, 535, 619, 289, 846, 665, 343, 927, 1095, 333, 45, 148, 1084, 854, 835, 491, 291, 129, 826, 116, 322, 870, 388, 728, 830, 261, 526, 558, 125, 101, 573, 69, 28, 109, 1061, 641, 1109, 1083, 476, 1068, 301, 803, 680, 626, 254, 801, 270, 851, 229, 587, 1071, 203, 255, 305, 253, 50, 575, 510, 189, 247, 117, 712, 283, 741, 389, 783, 1, 749, 288, 574, 102, 822, 1045, 682, 446, 233, 497, 400, 775, 556, 49, 537, 1101, 506, 120, 1030, 171, 788, 342, 519, 1020, 31, 236, 196, 935, 504, 295, 726, 690, 479, 1104, 655, 548, 834, 572, 118, 221, 1032, 1002, 1008, 199, 89, 932, 149, 947, 720, 838, 450, 123, 263, 1110, 923, 1082, 175, 369, 397, 632, 58, 1033, 86, 781, 368, 415, 639, 24, 44, 386, 487, 609, 320, 433, 798, 896, 538, 492, 472, 252, 466, 945, 708, 562, 542, 1042, 810, 761, 124, 1035, 938, 159, 546, 8, 1044, 296, 306, 829, 1089, 576, 396, 747, 876, 978, 844, 540, 983, 606, 1094, 38, 816, 222, 403, 393, 1052, 345, 692, 381, 346, 1007, 353, 314, 819, 158, 866, 72, 766, 930, 1056, 785, 974, 559, 651, 1021, 417, 791, 1079, 22, 438, 215, 145, 645, 588, 312, 1086, 670, 339, 32, 341, 1074, 162, 764, 532, 361, 2, 687, 378, 926, 99, 999, 410, 750, 882, 902, 915, 539, 693, 1065, 350, 463, 977, 456, 657, 658, 959, 1000, 577, 752, 707, 401, 471, 919, 336, 285, 383, 996, 436, 975, 56, 419, 351, 751, 250, 951, 43, 551, 357, 631, 880, 681, 467, 60, 358, 917, 553, 965, 180, 279, 416, 507, 269, 204, 943, 316, 937, 633, 869, 672, 349, 893, 377, 375, 57, 797, 73, 1060, 448, 65, 599, 299, 404, 1003, 1107, 130, 509, 1075, 168, 953, 1038, 136, 235, 290, 17, 48, 941, 437, 434, 533, 461, 549, 1093, 1025, 186, 868, 940, 266, 872, 371, 234, 991, 474, 636, 511, 855, 232, 637, 473, 286, 734, 586, 597, 359, 243, 185, 1051, 486, 1043, 992, 646, 610, 569, 227, 1046, 144, 565, 379, 782, 98, 635, 108, 406, 979, 683, 756, 884, 127, 321, 828, 352, 508, 166, 613, 9, 35, 928, 976, 87, 718, 454, 335, 564, 257, 172, 6, 901, 1054, 374, 557, 725, 904, 443, 956, 29, 529, 5, 304, 840, 478, 650, 1037, 402, 1004, 460, 839, 1047, 696, 957, 795, 676, 541, 107, 704, 115, 195, 857, 833, 1006, 581, 925, 376, 139, 664, 773, 719, 429, 628, 499, 331, 605, 469, 847, 886, 758, 892, 894, 480, 748, 807, 276, 732, 59, 1072, 716, 912, 83, 594, 207, 502, 850, 590, 151, 394, 414, 582, 104, 583, 735, 33, 176, 399, 409, 860, 311, 521, 823, 784, 1098, 554, 493, 853, 258, 1069, 622, 808, 982, 408, 36, 966, 425, 883, 786, 824, 298, 91, 1105, 423, 907, 294, 679, 71, 157, 614, 309, 275, 709, 1063, 827, 837, 620, 1014, 191, 169, 324, 93, 3, 929, 4, 293, 543, 858, 88, 1078, 652, 424, 701, 1073, 287, 560, 1070, 879, 898, 1108, 845, 498, 744, 213, 848, 547, 319, 1036, 237, 602, 760, 993, 667, 705, 998, 987, 955, 483, 516, 147, 427, 470, 934, 328, 694, 544, 585, 759, 517, 46, 444, 534, 515, 1081, 561, 241, 527, 70, 995, 264, 245, 372, 363, 809, 1077, 362, 501, 578, 1041, 308, 164, 274, 1096, 64, 638, 143, 42, 77, 900, 805, 722, 133, 260, 165, 407, 980, 223, 531, 1031, 85, 815, 625, 662, 849, 621, 802, 490, 395, 110, 774, 41, 861, 441, 23, 468, 14, 198, 142, 373, 267, 671, 994, 914, 608, 34, 380, 106, 1015, 277, 772, 19, 209, 113, 918, 218, 76, 961, 624, 300, 184, 697, 297, 219, 194, 439, 365, 871, 885, 330, 931, 366, 579, 421, 481, 128, 706, 754, 160, 489, 431, 477, 193, 913, 530, 271, 451, 852, 973, 804, 778, 458, 79, 97, 627, 334, 813, 698, 831, 520, 567, 272, 360, 220, 793, 1090, 739, 678, 385, 231, 895, 348, 887, 173, 1026, 457, 873, 84, 695, 317, 239, 563, 224, 1053, 740, 303, 787, 37, 910, 210, 405, 1028, 278, 170, 347, 598, 205, 249, 447, 226, 607, 206, 464, 717, 958, 432, 63, 668, 653, 713, 970, 445, 601, 67, 689, 382, 217, 715, 326, 452, 666, 922, 40, 738, 997, 796, 729, 596, 566, 202, 685, 522, 1085, 111, 950, 939, 122, 780, 1112, 53, 62, 179, 1064, 988, 776, 771, 244, 398, 812, 92, 736, 211, 355, 94, 1013, 817, 137, 954, 132, 412, 475, 821, 643, 265, 391, 323, 1097, 767, 100, 768, 1027, 119, 757, 126, 769, 78, 310, 647, 660, 141, 453, 589, 899, 178, 392, 150, 789, 384, 422, 700, 916, 10, 238, 890, 214, 66, 15, 737, 859, 656, 225, 989, 818, 26, 462, 746, 12, 268, 426, 765, 1049, 0, 634, 514, 95, 595, 568, 329, 55, 924, 528, 686, 865, 971, 903, 390, 197, 762, 580, 612, 1088, 82, 820, 495, 134, 21, 618, 513, 863, 984, 273, 155, 731, 897, 52, 972, 1024, 230, 292, 153, 435, 730, 1017, 420, 192, 570, 986, 177, 644, 47, 367, 494, 944, 1066, 20, 112, 1009, 302, 103, 877, 61, 188, 208, 960, 344, 370, 611, 600, 1023, 167, 779, 212, 921, 964, 630, 946, 743, 482, 721, 677, 418, 1029, 354, 777, 727, 518, 1011, 842, 449, 811, 571, 832, 699, 891, 1048, 80, 281, 523, 963, 936, 525, 430, 7, 623, 836, 121, 284, 242, 799, 1055, 933, 642, 500, 25, 888, 790, 792, 911, 703, 484, 604, 909, 723, 550, 1034, 949, 327, 488, 1039, 1010, 640, 661, 154, 1080, 763, 340, 592, 967, 1040, 616, 75, 669, 1050, 282, 262, 591, 187, 867, 663, 74, 337, 985, 905, 794, 325, 659, 338, 584, 1092, 814, 806, 81, 1001]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16MzwlM9oAqs"
      },
      "source": [
        "n = len(dataset) // 10 \n",
        "test_dataset = dataset[random_index[:n]]\n",
        "val_dataset = dataset[random_index[n:2 * n]]\n",
        "train_dataset = dataset[random_index[2 * n:]]\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGcYZA1WwGd1"
      },
      "source": [
        "# # The base line code\n",
        "\n",
        "# class Net(torch.nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, hidden_channels=32):\n",
        "#         super(Net, self).__init__()\n",
        "\n",
        "#         self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "#         num_nodes = ceil(0.5 * average_nodes)\n",
        "#         self.pool1 = Linear(hidden_channels, num_nodes)\n",
        "\n",
        "#         self.conv2 = DenseGraphConv(hidden_channels, hidden_channels)\n",
        "#         num_nodes = ceil(0.5 * num_nodes)\n",
        "#         self.pool2 = Linear(hidden_channels, num_nodes)\n",
        "\n",
        "#         self.conv3 = DenseGraphConv(hidden_channels, hidden_channels)\n",
        "\n",
        "#         self.lin1 = Linear(hidden_channels, hidden_channels)\n",
        "#         self.lin2 = Linear(hidden_channels, out_channels)\n",
        "\n",
        "#     def forward(self, x, edge_index, batch):\n",
        "#         # print('first x: ', x.shape)\n",
        "#         x = F.relu(self.conv1(x, edge_index))\n",
        "#         # print('x shape: ', x.shape) # x shape:  torch.Size([176, 32])\n",
        "\n",
        "#         x, mask = to_dense_batch(x, batch)\n",
        "#         # print('x shape: ', x.shape) # x shape:  torch.Size([8, 47, 32])\n",
        "#         adj = to_dense_adj(edge_index, batch) \n",
        "\n",
        "#         s = self.pool1(x)\n",
        "#         # print('s shape: ', s.shape) # s shape:  torch.Size([8, 47, 20])\n",
        "#         x, adj, mc1, o1 = dense_diff_pool(x, adj, s, mask)\n",
        "#         # print('x shape: ', x.shape) # x shape:  torch.Size([8, 20, 32])\n",
        "\n",
        "#         x = F.relu(self.conv2(x, adj))\n",
        "#         # print('x shape: ', x.shape) # x shape:  torch.Size([8, 20, 32])\n",
        "#         s = self.pool2(x)\n",
        "#         # print('s shape: ', s.shape) # s shape:  torch.Size([8, 20, 10])\n",
        "\n",
        "#         x, adj, mc2, o2 = dense_diff_pool(x, adj, s)\n",
        "#         # print('x shape: ', x.shape) # x shape:  torch.Size([8, 10, 32])\n",
        "\n",
        "#         x = self.conv3(x, adj)\n",
        "#         # print('x shape: ', x.shape) # x shape:  torch.Size([8, 10, 32])\n",
        "\n",
        "#         x = x.mean(dim=1)\n",
        "#         # print('x shape: ', x.shape) # x shape:  torch.Size([8, 32])\n",
        "#         x = F.relu(self.lin1(x))\n",
        "#         # print('x shape: ', x.shape) # x shape:  torch.Size([8, 32])\n",
        "#         x = self.lin2(x)\n",
        "#         # print('x shape: ', x.shape) # x shape:  torch.Size([8, 2])\n",
        "#         return F.log_softmax(x, dim=-1), mc1 + mc2, o1 + o2"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahZAcmiWUnwZ"
      },
      "source": [
        "# The proposed code utlilizing Graph UNet\n",
        "\n",
        "from torch_geometric.nn import GraphUNet\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, hidden_channels=32):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = GraphUNet(in_channels, hidden_channels, hidden_channels, depth=3)\n",
        "        num_nodes = ceil(0.5 * average_nodes)\n",
        "        self.pool1 = Linear(hidden_channels, num_nodes)\n",
        "        self.conv2 = DenseGraphConv(hidden_channels, hidden_channels)\n",
        "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
        "        self.lin2 = Linear(hidden_channels, out_channels)\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x, mask = to_dense_batch(x, batch)\n",
        "        adj = to_dense_adj(edge_index, batch) \n",
        "        s = self.pool1(x)\n",
        "        x, adj, mc1, o1 = dense_diff_pool(x, adj, s, mask)\n",
        "        x = self.conv2(x, adj)\n",
        "        x = x.mean(dim=1)\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = self.lin2(x)\n",
        "        return F.log_softmax(x, dim=-1), mc1, o1 #, mc1 + mc2, o1 + o2"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh094rWnP4pU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67a8ed27-1afa-42e0-c66e-a8157a882d84"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net(dataset.num_features, dataset.num_classes).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    loss_all = 0\n",
        "\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out, mc_loss, o_loss = model(data.x, data.edge_index, data.batch)\n",
        "        loss = F.nll_loss(out, data.y.view(-1)) + mc_loss + o_loss\n",
        "        loss.backward()\n",
        "        loss_all += data.y.size(0) * loss.item()\n",
        "        optimizer.step()\n",
        "    return loss_all / len(train_dataset)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def valid(loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    loss_all = 0\n",
        "\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        pred, mc_loss, o_loss = model(data.x, data.edge_index, data.batch)\n",
        "        loss = F.nll_loss(pred, data.y.view(-1)) + mc_loss + o_loss\n",
        "        loss_all += data.y.size(0) * loss.item()\n",
        "        correct += pred.max(dim=1)[1].eq(data.y.view(-1)).sum().item()\n",
        "\n",
        "    return loss_all / len(loader.dataset), correct / len(loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_eval(loader, model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    loss_all = 0\n",
        "\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        pred, mc_loss, o_loss = model(data.x, data.edge_index, data.batch)\n",
        "        loss = F.nll_loss(pred, data.y.view(-1)) + mc_loss + o_loss\n",
        "        loss_all += data.y.size(0) * loss.item()\n",
        "        correct += pred.max(dim=1)[1].eq(data.y.view(-1)).sum().item()\n",
        "\n",
        "    return loss_all / len(loader.dataset), correct / len(loader.dataset)\n",
        "\n",
        "best_val_acc = test_acc = 0\n",
        "best_val_loss = float('inf')\n",
        "start_patience = 50\n",
        "patience = 50\n",
        "for epoch in range(1, 200):\n",
        "    train_loss = train(epoch)\n",
        "    _, train_acc = valid(train_loader)\n",
        "    val_loss, val_acc = valid(val_loader)\n",
        "    if val_loss < best_val_loss and val_acc > best_val_acc:\n",
        "        test_loss, test_acc = valid(test_loader)\n",
        "        best_val_loss = val_loss\n",
        "        best_val_acc = val_acc\n",
        "        patience = start_patience\n",
        "        eval_model = model\n",
        "    else:\n",
        "        patience -= 1\n",
        "        if patience == 0:\n",
        "            break\n",
        "    print('Epoch: {:03d}, '\n",
        "          'Train Loss: {:.3f}, Train Acc: {:.3f}, '\n",
        "          'Val Loss: {:.3f}, Val Acc: {:.3f}, '\n",
        "          'Test Loss: {:.3f}, Test Acc: {:.3f}'.format(epoch, train_loss,\n",
        "                                                       train_acc, val_loss,\n",
        "                                                       val_acc, test_loss,\n",
        "                                                       test_acc))\n",
        "\n",
        "fin_test_loss, fin_test_acc = test_eval(test_loader, eval_model)\n",
        "print('Final Test Loss: {:.3f}, Test Acc: {:.3f}'.format(test_loss, test_acc))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train Loss: 1.846, Train Acc: 0.714, Val Loss: 1.944, Val Acc: 0.739, Test Loss: 1.867, Test Acc: 0.739\n",
            "Epoch: 002, Train Loss: 1.831, Train Acc: 0.724, Val Loss: 1.941, Val Acc: 0.748, Test Loss: 1.854, Test Acc: 0.748\n",
            "Epoch: 003, Train Loss: 1.820, Train Acc: 0.734, Val Loss: 1.936, Val Acc: 0.730, Test Loss: 1.854, Test Acc: 0.748\n",
            "Epoch: 004, Train Loss: 1.810, Train Acc: 0.730, Val Loss: 1.928, Val Acc: 0.721, Test Loss: 1.854, Test Acc: 0.748\n",
            "Epoch: 005, Train Loss: 1.799, Train Acc: 0.733, Val Loss: 1.919, Val Acc: 0.721, Test Loss: 1.854, Test Acc: 0.748\n",
            "Epoch: 006, Train Loss: 1.787, Train Acc: 0.735, Val Loss: 1.906, Val Acc: 0.721, Test Loss: 1.854, Test Acc: 0.748\n",
            "Epoch: 007, Train Loss: 1.773, Train Acc: 0.736, Val Loss: 1.889, Val Acc: 0.730, Test Loss: 1.854, Test Acc: 0.748\n",
            "Epoch: 008, Train Loss: 1.754, Train Acc: 0.735, Val Loss: 1.867, Val Acc: 0.730, Test Loss: 1.854, Test Acc: 0.748\n",
            "Epoch: 009, Train Loss: 1.732, Train Acc: 0.734, Val Loss: 1.835, Val Acc: 0.730, Test Loss: 1.854, Test Acc: 0.748\n",
            "Epoch: 010, Train Loss: 1.701, Train Acc: 0.733, Val Loss: 1.795, Val Acc: 0.748, Test Loss: 1.854, Test Acc: 0.748\n",
            "Epoch: 011, Train Loss: 1.665, Train Acc: 0.738, Val Loss: 1.748, Val Acc: 0.757, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 012, Train Loss: 1.623, Train Acc: 0.740, Val Loss: 1.698, Val Acc: 0.748, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 013, Train Loss: 1.581, Train Acc: 0.743, Val Loss: 1.652, Val Acc: 0.748, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 014, Train Loss: 1.540, Train Acc: 0.752, Val Loss: 1.604, Val Acc: 0.712, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 015, Train Loss: 1.501, Train Acc: 0.747, Val Loss: 1.544, Val Acc: 0.748, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 016, Train Loss: 1.444, Train Acc: 0.749, Val Loss: 1.479, Val Acc: 0.757, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 017, Train Loss: 1.389, Train Acc: 0.744, Val Loss: 1.414, Val Acc: 0.748, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 018, Train Loss: 1.341, Train Acc: 0.741, Val Loss: 1.359, Val Acc: 0.748, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 019, Train Loss: 1.298, Train Acc: 0.749, Val Loss: 1.309, Val Acc: 0.748, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 020, Train Loss: 1.239, Train Acc: 0.749, Val Loss: 1.251, Val Acc: 0.748, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 021, Train Loss: 1.207, Train Acc: 0.752, Val Loss: 1.209, Val Acc: 0.757, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 022, Train Loss: 1.275, Train Acc: 0.691, Val Loss: 1.765, Val Acc: 0.739, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 023, Train Loss: 1.639, Train Acc: 0.716, Val Loss: 1.255, Val Acc: 0.658, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 024, Train Loss: 1.220, Train Acc: 0.716, Val Loss: 1.229, Val Acc: 0.613, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 025, Train Loss: 1.252, Train Acc: 0.733, Val Loss: 1.207, Val Acc: 0.676, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 026, Train Loss: 1.225, Train Acc: 0.738, Val Loss: 1.188, Val Acc: 0.667, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 027, Train Loss: 1.200, Train Acc: 0.738, Val Loss: 1.177, Val Acc: 0.712, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 028, Train Loss: 1.187, Train Acc: 0.741, Val Loss: 1.161, Val Acc: 0.703, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 029, Train Loss: 1.169, Train Acc: 0.745, Val Loss: 1.148, Val Acc: 0.721, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 030, Train Loss: 1.157, Train Acc: 0.747, Val Loss: 1.139, Val Acc: 0.730, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 031, Train Loss: 1.144, Train Acc: 0.745, Val Loss: 1.121, Val Acc: 0.730, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 032, Train Loss: 1.129, Train Acc: 0.744, Val Loss: 1.108, Val Acc: 0.721, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 033, Train Loss: 1.119, Train Acc: 0.746, Val Loss: 1.093, Val Acc: 0.739, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 034, Train Loss: 1.107, Train Acc: 0.750, Val Loss: 1.075, Val Acc: 0.739, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 035, Train Loss: 1.095, Train Acc: 0.751, Val Loss: 1.058, Val Acc: 0.739, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 036, Train Loss: 1.080, Train Acc: 0.750, Val Loss: 1.047, Val Acc: 0.748, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 037, Train Loss: 1.032, Train Acc: 0.746, Val Loss: 1.031, Val Acc: 0.730, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 038, Train Loss: 1.017, Train Acc: 0.745, Val Loss: 1.014, Val Acc: 0.721, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 039, Train Loss: 1.004, Train Acc: 0.749, Val Loss: 1.000, Val Acc: 0.730, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 040, Train Loss: 0.994, Train Acc: 0.751, Val Loss: 0.982, Val Acc: 0.712, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 041, Train Loss: 0.978, Train Acc: 0.750, Val Loss: 0.973, Val Acc: 0.739, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 042, Train Loss: 0.993, Train Acc: 0.758, Val Loss: 0.964, Val Acc: 0.721, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 043, Train Loss: 0.950, Train Acc: 0.759, Val Loss: 0.952, Val Acc: 0.739, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 044, Train Loss: 0.966, Train Acc: 0.750, Val Loss: 0.950, Val Acc: 0.721, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 045, Train Loss: 0.932, Train Acc: 0.758, Val Loss: 0.944, Val Acc: 0.712, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 046, Train Loss: 0.975, Train Acc: 0.756, Val Loss: 0.939, Val Acc: 0.703, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 047, Train Loss: 0.926, Train Acc: 0.750, Val Loss: 0.929, Val Acc: 0.712, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 048, Train Loss: 0.949, Train Acc: 0.751, Val Loss: 0.930, Val Acc: 0.712, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 049, Train Loss: 0.945, Train Acc: 0.754, Val Loss: 0.926, Val Acc: 0.703, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 050, Train Loss: 0.900, Train Acc: 0.753, Val Loss: 0.919, Val Acc: 0.694, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 051, Train Loss: 0.900, Train Acc: 0.752, Val Loss: 0.911, Val Acc: 0.703, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 052, Train Loss: 0.904, Train Acc: 0.744, Val Loss: 0.898, Val Acc: 0.757, Test Loss: 1.596, Test Acc: 0.838\n",
            "Epoch: 053, Train Loss: 0.891, Train Acc: 0.743, Val Loss: 0.887, Val Acc: 0.766, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 054, Train Loss: 1.072, Train Acc: 0.673, Val Loss: 2.059, Val Acc: 0.712, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 055, Train Loss: 1.878, Train Acc: 0.749, Val Loss: 0.992, Val Acc: 0.694, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 056, Train Loss: 1.034, Train Acc: 0.747, Val Loss: 0.921, Val Acc: 0.748, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 057, Train Loss: 0.911, Train Acc: 0.750, Val Loss: 0.917, Val Acc: 0.730, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 058, Train Loss: 0.896, Train Acc: 0.746, Val Loss: 0.910, Val Acc: 0.739, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 059, Train Loss: 0.890, Train Acc: 0.747, Val Loss: 0.908, Val Acc: 0.739, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 060, Train Loss: 0.882, Train Acc: 0.743, Val Loss: 0.907, Val Acc: 0.730, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 061, Train Loss: 0.870, Train Acc: 0.744, Val Loss: 0.898, Val Acc: 0.739, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 062, Train Loss: 0.861, Train Acc: 0.743, Val Loss: 0.894, Val Acc: 0.739, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 063, Train Loss: 0.854, Train Acc: 0.743, Val Loss: 0.887, Val Acc: 0.739, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 064, Train Loss: 0.845, Train Acc: 0.747, Val Loss: 0.879, Val Acc: 0.730, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 065, Train Loss: 0.838, Train Acc: 0.749, Val Loss: 0.869, Val Acc: 0.721, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 066, Train Loss: 0.828, Train Acc: 0.751, Val Loss: 0.858, Val Acc: 0.721, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 067, Train Loss: 0.817, Train Acc: 0.754, Val Loss: 0.842, Val Acc: 0.721, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 068, Train Loss: 0.825, Train Acc: 0.754, Val Loss: 0.831, Val Acc: 0.721, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 069, Train Loss: 0.787, Train Acc: 0.749, Val Loss: 0.811, Val Acc: 0.721, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 070, Train Loss: 0.771, Train Acc: 0.747, Val Loss: 0.793, Val Acc: 0.730, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 071, Train Loss: 0.764, Train Acc: 0.751, Val Loss: 0.772, Val Acc: 0.730, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 072, Train Loss: 0.751, Train Acc: 0.749, Val Loss: 0.746, Val Acc: 0.730, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 073, Train Loss: 0.726, Train Acc: 0.746, Val Loss: 0.728, Val Acc: 0.739, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 074, Train Loss: 0.783, Train Acc: 0.745, Val Loss: 0.723, Val Acc: 0.730, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 075, Train Loss: 0.701, Train Acc: 0.746, Val Loss: 0.703, Val Acc: 0.730, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 076, Train Loss: 0.688, Train Acc: 0.750, Val Loss: 0.689, Val Acc: 0.730, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 077, Train Loss: 0.682, Train Acc: 0.750, Val Loss: 0.684, Val Acc: 0.739, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 078, Train Loss: 0.669, Train Acc: 0.750, Val Loss: 0.668, Val Acc: 0.739, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 079, Train Loss: 0.670, Train Acc: 0.751, Val Loss: 0.666, Val Acc: 0.739, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 080, Train Loss: 0.654, Train Acc: 0.755, Val Loss: 0.654, Val Acc: 0.739, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 081, Train Loss: 0.836, Train Acc: 0.756, Val Loss: 0.648, Val Acc: 0.703, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 082, Train Loss: 1.004, Train Acc: 0.752, Val Loss: 0.676, Val Acc: 0.712, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 083, Train Loss: 0.638, Train Acc: 0.752, Val Loss: 0.649, Val Acc: 0.721, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 084, Train Loss: 0.626, Train Acc: 0.755, Val Loss: 0.642, Val Acc: 0.730, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 085, Train Loss: 0.618, Train Acc: 0.752, Val Loss: 0.637, Val Acc: 0.730, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 086, Train Loss: 0.613, Train Acc: 0.753, Val Loss: 0.633, Val Acc: 0.730, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 087, Train Loss: 0.606, Train Acc: 0.747, Val Loss: 0.629, Val Acc: 0.739, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 088, Train Loss: 0.606, Train Acc: 0.752, Val Loss: 0.623, Val Acc: 0.730, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 089, Train Loss: 0.602, Train Acc: 0.752, Val Loss: 0.623, Val Acc: 0.730, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 090, Train Loss: 0.602, Train Acc: 0.755, Val Loss: 0.620, Val Acc: 0.730, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 091, Train Loss: 0.600, Train Acc: 0.755, Val Loss: 0.619, Val Acc: 0.730, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 092, Train Loss: 0.597, Train Acc: 0.754, Val Loss: 0.616, Val Acc: 0.730, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 093, Train Loss: 0.592, Train Acc: 0.755, Val Loss: 0.613, Val Acc: 0.730, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 094, Train Loss: 0.590, Train Acc: 0.751, Val Loss: 0.608, Val Acc: 0.757, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 095, Train Loss: 0.587, Train Acc: 0.753, Val Loss: 0.606, Val Acc: 0.739, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 096, Train Loss: 0.583, Train Acc: 0.754, Val Loss: 0.605, Val Acc: 0.748, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 097, Train Loss: 0.582, Train Acc: 0.754, Val Loss: 0.604, Val Acc: 0.730, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 098, Train Loss: 0.580, Train Acc: 0.751, Val Loss: 0.597, Val Acc: 0.748, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 099, Train Loss: 0.580, Train Acc: 0.751, Val Loss: 0.595, Val Acc: 0.739, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 100, Train Loss: 0.580, Train Acc: 0.751, Val Loss: 0.593, Val Acc: 0.730, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 101, Train Loss: 0.576, Train Acc: 0.753, Val Loss: 0.593, Val Acc: 0.721, Test Loss: 0.698, Test Acc: 0.865\n",
            "Epoch: 102, Train Loss: 0.574, Train Acc: 0.755, Val Loss: 0.591, Val Acc: 0.730, Test Loss: 0.698, Test Acc: 0.865\n",
            "Final Test Loss: 0.698, Test Acc: 0.865\n"
          ]
        }
      ]
    }
  ]
}
